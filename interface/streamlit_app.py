#!/usr/bin/env python3
"""
Streamlit Web Interface for Armenian Labor Law Q&A
"""

import sys
import json
from pathlib import Path
import streamlit as st

# Add project root to path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from src.retrieval.bm25_retriever import BM25Retriever
from src.retrieval.dense_retriever import DenseRetriever
from src.generation.rag_pipeline import RAGPipeline
from src.generation.generator import LLMGenerator


# Page config
st.set_page_config(
    page_title="Armenian Labor Law Q&A",
    page_icon="üá¶üá≤",
    layout="wide"
)

# Title
st.title("üá¶üá≤ ’Ä’°’µ’°’Ω’ø’°’∂’´ ‘±’∑’≠’°’ø’°’∂÷Ñ’°’µ’´’∂ ÷Ö÷Ä’•’∂’Ω’£’´÷Ä÷Ñ")
st.subheader("’Ä’°÷Ä÷Å’•÷Ä ÷á ’ä’°’ø’°’Ω’≠’°’∂’∂’•÷Ä (RAG System)")

# Sidebar
with st.sidebar:
    st.header("‚öôÔ∏è ‘ø’°÷Ä’£’°’æ’∏÷Ä’∏÷Ç’¥’∂’•÷Ä")
    
    retrieval_method = st.selectbox(
        "’à÷Ä’∏’∂’¥’°’∂ ’¥’•’©’∏’§:",
        ["BM25 (Keywords)", "Dense (Armenian Embeddings)", "Hybrid (Both)"],
        index=0
    )
    
    top_k = st.slider("’Ä’∏’§’æ’°’Æ’∂’•÷Ä’´ ÷Ñ’°’∂’°’Ø:", min_value=1, max_value=10, value=3)
    
    st.markdown("---")
    st.markdown("### üìä ’Ä’°’¥’°’Ø’°÷Ä’£’´ ’ø’•’≤’•’Ø’∏÷Ç’©’µ’∏÷Ç’∂’∂’•÷Ä")
    st.markdown("- **’Ä’∏’§’æ’°’Æ’∂’•÷Ä:** 286")
    st.markdown("- **’Ñ’∏’§’•’¨:** NVIDIA Llama 3.1-70B")
    st.markdown("- **‘º’•’¶’∏÷Ç:** ’Ä’°’µ’•÷Ä’•’∂")


@st.cache_resource
def load_rag_pipeline(method):
    """Load RAG pipeline (cached)."""
    # Load chunks
    chunks_file = project_root / "data" / "chunks" / "labor_law_chunks.json"
    with open(chunks_file, 'r', encoding='utf-8') as f:
        chunks = json.load(f)
    
    # Initialize retriever
    if "BM25" in method:
        retriever = BM25Retriever(chunks)
        index_path = project_root / "indices" / "bm25" / "bm25_index.pkl"
        retriever.load_index(str(index_path))
    elif "Dense" in method:
        retriever = DenseRetriever(chunks)
        index_path = project_root / "indices" / "dense"
        retriever.load_index(str(index_path))
    else:  # Hybrid
        # TODO: Implement hybrid
        retriever = BM25Retriever(chunks)
        index_path = project_root / "indices" / "bm25" / "bm25_index.pkl"
        retriever.load_index(str(index_path))
    
    # Initialize generator
    api_key = "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"
    generator = LLMGenerator(
        model_name="meta/llama-3.1-70b-instruct",
        provider="nvidia",
        api_key=api_key,
        max_tokens=1000,
        temperature=0.1
    )
    
    # Create RAG pipeline
    return RAGPipeline(retriever=retriever, generator=generator)


# Load pipeline
rag_pipeline = load_rag_pipeline(retrieval_method)

# Main chat interface
st.markdown("### üí¨ ’Ä’°÷Ä÷Å÷Ä’•÷Ñ ’´’∂’± ’°’∑’≠’°’ø’°’∂÷Ñ’°’µ’´’∂ ’´÷Ä’°’æ’∏÷Ç’∂÷Ñ’´ ’¥’°’Ω’´’∂:")

# Example questions
with st.expander("üìù ’ï÷Ä’´’∂’°’Ø ’∞’°÷Ä÷Å’•÷Ä"):
    st.markdown("""
    - ’à÷Ä’∏’û’∂÷Ñ ’•’∂ ’∂’æ’°’¶’°’£’∏÷Ç’µ’∂ ’°’∑’≠’°’ø’°’æ’°÷Ä’±’´ ’Ø’°’∂’∏’∂’∂’•÷Ä’®÷â
    - ’î’°’∂’´’û ’°÷Ä’±’°’Ø’∏÷Ç÷Ä’§’°’µ’´’∂ ÷Ö÷Ä ’Ø’°÷â
    - ‘ª’∂’π’∫’•’û’Ω ’ß ’Ω’°’∞’¥’°’∂’æ’∏÷Ç’¥ ’£’∏÷Ä’Æ’∏÷Ç’≤’¥’°’∂ ÷Ö÷Ä’°’∫’°’∞’´’Ø’®÷â
    - ‘ª’û’∂’π ’´÷Ä’°’æ’∏÷Ç’∂÷Ñ’∂’•÷Ä ’∏÷Ç’∂’´ ’°’∑’≠’°’ø’∏’≤’® ’•÷Ä’¢ ’´÷Ä’•’∂ ’Ø÷Ä’≥’°’ø’∏÷Ç’¥ ’•’∂÷â
    - ‘ª’∂’π ’ß ’°’Ω’∏÷Ç’¥ ’Ä’∏’§’æ’°’Æ 1-’´’∂ ’∞’∏’§’æ’°’Æ’®÷â
    """)

# Question input
question = st.text_input(
    "’Å’•÷Ä ’∞’°÷Ä÷Å’®:",
    placeholder="‘≥÷Ä’•÷Ñ ’±’•÷Ä ’∞’°÷Ä÷Å’® ’∞’°’µ’•÷Ä’•’∂...",
    key="question_input"
)

# Search button
if st.button("üîç ’ì’∂’ø÷Ä’•’¨", type="primary") or question:
    if question:
        with st.spinner('üîç ’ì’∂’ø÷Ä’∏÷Ç’¥ ’•’¥ ’∞’°’¥’°’∫’°’ø’°’Ω’≠’°’∂ ’∞’∏’§’æ’°’Æ’∂’•÷Ä...'):
            try:
                # Get answer
                result = rag_pipeline.answer_question(
                    question,
                    top_k=top_k,
                    return_context=True
                )
                
                # Display answer
                st.markdown("---")
                st.markdown("### üí° ’ä‘±’è‘±’ç‘Ω‘±’Ü:")
                st.success(result['answer'])
                
                # Display retrieved articles
                st.markdown("---")
                st.markdown("### üìä ‘≥’ø’∂’æ’°’Æ ’∞’∏’§’æ’°’Æ’∂’•÷Ä:")
                
                cols = st.columns(3)
                for i, (article, score) in enumerate(zip(result['article_numbers'], result['scores'])):
                    with cols[i % 3]:
                        st.metric(
                            label=f"’Ä’∏’§’æ’°’Æ {article}",
                            value=f"{score:.2f}",
                            delta=f"#{i+1}"
                        )
                
                # Show context in expander
                with st.expander("üìÑ ‘¥’´’ø’•’¨ ’£’ø’∂’æ’°’Æ ’∞’∏’§’æ’°’Æ’∂’•÷Ä’´ ’ø’•÷Ñ’Ω’ø’®"):
                    for i, chunk in enumerate(result['retrieved_chunks'], 1):
                        st.markdown(f"**’Ä’∏’§’æ’°’Æ {chunk.get('article_number')}:**")
                        st.text(chunk['text'][:500] + "...")
                        st.markdown("---")
                
            except Exception as e:
                st.error(f"‚ùå ’ç’≠’°’¨: {e}")
    else:
        st.warning("‚ö†Ô∏è ‘Ω’∂’§÷Ä’∏÷Ç’¥ ’•’∂÷Ñ ’¥’∏÷Ç’ø÷Ñ’°’£÷Ä’•’¨ ’∞’°÷Ä÷Å")

# Footer
st.markdown("---")
st.markdown("""
<div style='text-align: center; color: gray;'>
    <p>üá¶üá≤ Armenian Labor Law RAG System | Powered by NVIDIA Llama 3.1-70B</p>
    <p>Using {method} | 286 Articles Indexed</p>
</div>
""".format(method=retrieval_method), unsafe_allow_html=True)
