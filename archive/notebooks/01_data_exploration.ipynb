{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "from pathlib import Path\n",
        "import re\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.style.use('default')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "DATA_DIR = Path('../data')\n",
        "PROCESSED_DIR = DATA_DIR / 'processed'\n",
        "text_file = PROCESSED_DIR / 'labor_law.txt'\n",
        "metadata_file = PROCESSED_DIR / 'labor_law_metadata.json'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Load dataand metadata**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üìä Basic Info:\n",
            "   - Total Pages: 86\n",
            "   - Total Characters: 335,890\n",
            "   - Total Words: 39,576\n",
            "   - Average chars/page: 3,905\n"
          ]
        }
      ],
      "source": [
        "with open(text_file, 'r', encoding = 'utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "with open(metadata_file, 'r', encoding = 'utf-8') as f:\n",
        "    metadata = json.load(f)\n",
        "\n",
        "print(f\"\\nüìä Basic Info:\")\n",
        "print(f\"   - Total Pages: {metadata['total_pages']}\")\n",
        "print(f\"   - Total Characters: {metadata['total_chars']:,}\")\n",
        "print(f\"   - Total Words: {metadata['total_words']:,}\")\n",
        "print(f\"   - Average chars/page: {metadata['total_chars'] // metadata['total_pages']:,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "’Ä’Ä ‘±’¶’£’°’µ’´’∂ ‘∫’∏’≤’∏’æ, ’ï÷Ä’•’∂’Ω’£’´÷Ä÷Ñ N ’Ä’ï-124-’Ü ‘∏’∂’§’∏÷Ç’∂’æ’•’¨ ’ß. 09.11.2004\n",
            "’Ä‘±’Ö‘±’ç’è‘±’Ü‘ª ’Ä‘±’Ü’ê‘±’ä‘µ’è’à’í‘π’Ö‘±’Ü ‘±’á‘Ω‘±’è‘±’Ü’î‘±’Ö‘ª’Ü ’ï’ê‘µ’Ü’ç‘≥‘ª’ê’î ’ç’ø’∏÷Ä’°’£÷Ä’æ’•’¨ ’ß. 14.12.2004\n",
            "’à’í’™’´ ’¥’•’ª ’ß ’¥’ø’•’¨. 21.06.2005\n",
            "01.01.2026 -\n",
            "’Ä‘±’Ö‘±’ç’è‘±’Ü‘ª ’Ä‘±’Ü’ê‘±’ä‘µ’è’à’í‘π’Ö‘±’Ü\n",
            "‘±’á‘Ω‘±’è‘±’Ü’î‘±’Ö‘ª’Ü ’ï’ê‘µ’Ü’ç‘≥‘ª’ê’î\n",
            "‘∏’∂’§’∏÷Ç’∂’æ’°’Æ ’ß 2004 ’©’æ’°’Ø’°’∂’´ ’∂’∏’µ’•’¥’¢’•÷Ä’´ 9-’´’∂\n",
            "‘≤‘±‘∫‘ª’Ü 1.\n",
            "‘∏’Ü‘¥’Ä‘±’Ü’à’í’ê ‘¥’ê’à’í’Ö‘π’Ü‘µ’ê\n",
            "‘≥‘º’à’í‘Ω 1.\n",
            "‘±’á‘Ω‘±’è‘±’Ü’î‘±’Ö‘ª’Ü ’ï’ê‘µ’Ü’ç‘¥’ê’à’í‘π’Ö’à’í’Ü‘∏ ‘µ’é ‘¥’ê‘±’Ü’à’é ‘ø‘±’ê‘≥‘±’é’à’ê’é’à’Ç ’Ä‘±’ê‘±‘≤‘µ’ê’à’í‘π’Ö’à’í’Ü’Ü‘µ’ê‘∏\n",
            "’Ä’∏’§’æ’°’Æ 1.’Ä’°’µ’°’Ω’ø’°’∂’´ ’Ä’°’∂÷Ä’°’∫’•’ø’∏÷Ç’©’µ’°’∂ ’°’∑’≠’°’ø’°’∂÷Ñ’°’µ’´’∂ ÷Ö÷Ä’•’∂’Ω’£÷Ä÷Ñ’∏’æ ’Ø’°÷Ä’£’°’æ’∏÷Ä’æ’∏’≤ ’∞’°÷Ä’°’¢’•÷Ä’∏÷Ç’©’µ’∏÷Ç’∂’∂’•÷Ä’®\n",
            "1. ’ç’∏÷Ç’µ’∂ ÷Ö÷Ä’•’∂’Ω’£’´÷Ä÷Ñ’® ’Ø’°÷Ä’£’°’æ’∏÷Ä’∏÷Ç’¥ ’ß ’Ø’∏’¨’•’Ø’ø’´’æ ÷á ’°’∂’∞’°’ø’°’Ø’°’∂ ’°’∑’≠’°’ø’°’∂÷Ñ’°’µ’´’∂ ’∞’°÷Ä’°’¢’•÷Ä’∏÷Ç’©’µ’∏÷Ç’∂’∂’•÷Ä’®, ’Ω’°’∞’¥’°’∂’∏÷Ç’¥ ’ß ’°’µ’§\n",
            "’∞’°÷Ä’°’¢’•÷Ä’∏÷Ç’©’µ’∏÷Ç’∂’∂’•÷Ä’´ ’Æ’°’£’¥’°’∂, ÷É’∏÷É’∏’≠’¥’°’∂ ÷á ’§’°’§’°÷Ä’¥’°’∂ ’∞’´’¥÷Ñ’•÷Ä’∂ ’∏÷Ç ’´÷Ä’°’Ø’°’∂’°÷Å’¥’°’∂ ’Ø’°÷Ä’£’®, ’°’∑’≠’°’ø’°’∂÷Ñ’°’µ’´’∂\n",
            "’∞’°÷Ä’°’¢’•÷Ä’∏÷Ç’©’µ’∏÷Ç’∂’∂’•÷Ä’´ ’Ø’∏’≤’¥’•÷Ä’´ ’´÷Ä’°’æ’∏÷Ç’∂÷Ñ’∂’•÷Ä’∂ ’∏÷Ç ’∫’°÷Ä’ø’°’Ø’°’∂’∏÷Ç’©’µ’∏÷Ç’∂’∂’•÷Ä’®, ’∫’°’ø’°’Ω’≠’°’∂’°’ø’æ’∏÷Ç’©’µ’∏÷Ç’∂’®, ’´’∂’π’∫’•’Ω ’∂’°÷á\n",
            "’°’∑’≠’°’ø’∏’≤’∂’•÷Ä’´ ’°’∂’æ’ø’°’∂’£’∏÷Ç’©’µ’°’∂ ’°’∫’°’∞’∏’æ’¥’°’∂ ’∏÷Ç ’°’º’∏’≤’ª’∏÷Ç’©’µ’°’∂ ’∫’°’∞’∫’°’∂’¥’°’∂ ’∫’°’µ’¥’°’∂’∂’•÷Ä’®:\n",
            "2. ‘±’∑’≠’°’ø’°’∂÷Ñ’°’µ’´’∂ ’∞’°÷Ä’°’¢’•÷Ä’∏÷Ç’©’µ’∏÷Ç’∂’∂’•÷Ä’´ ’°’º’°’∂’±’´’∂ ’¢’∂’°’£’°’æ’°’º’∂’•÷Ä’´ ’Ø’°÷Ä’£’°’æ’∏÷Ä’¥’°’∂ ’°’º’°’∂’±’∂’°’∞’°’ø’Ø’∏÷Ç’©’µ’∏÷Ç’∂’∂’•÷Ä’® ’Ø’°÷Ä’∏’≤\n",
            "’•’∂ ’Ω’°’∞’¥’°’∂’æ’•’¨ ÷Ö÷Ä’•’∂÷Ñ’∏’æ:\n",
            "’Ä’∏’§’æ’°’Æ 2.‘±’∑’≠’°’ø’°’∂÷Ñ’°’µ’´’∂ ÷Ö÷Ä’•’∂’Ω’§÷Ä’∏÷Ç’©’µ’°’∂ ’∂’∫’°’ø’°’Ø’®\n",
            "‘±’∑’≠’°’ø’°’∂÷Ñ’°’µ’´’∂\n"
          ]
        }
      ],
      "source": [
        "print(text[:1000])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "' ’∫’°÷Ä’°’∫’∏÷Ç÷Ä’§’´ ’°’¥’¢’∏’≤’ª\\n’™’°’¥’°’∂’°’Ø’°’∞’°’ø’æ’°’Æ’´ ’°’∑’≠’°’ø’°’∂÷Ñ’°’µ’´’∂ ÷Ö÷Ä’•÷Ä’´ ÷Ñ’°’∂’°’Ø’∏’æ ’¢’°’¶’¥’°’∫’°’ø’Ø’•’¨’∏÷Ç ’¥’´’ª’∏÷Å’∏’æ:\\n(265-÷Ä’§ ’∞’∏’§’æ’°’Æ’® ÷É’∏÷É. 24.06.10 ’Ä’ï-117-’Ü, 12.03.14 ’Ä’ï-5-’Ü, 03.05.23 ’Ä’ï-160-’Ü (÷Ö÷Ä’•’∂÷Ñ’∂ ’∏÷Ç’∂’´ ’•’¶÷Ä’°÷É’°’Ø’´’π ’¥’°’Ω ÷á ’°’∂÷Å’∏÷Ç’¥’°’µ’´’∂\\n’§÷Ä’∏÷Ç’µ’©’∂’•÷Ä), 02.10.24 ’Ä’ï-364-’Ü ÷Ö÷Ä’•’∂÷Ñ’∂’•÷Ä )\\n’Ä’∏’§’æ’°’Æ 266.‘±’∑’≠’°’ø’°’∂÷Ñ’°’µ’´’∂ ’æ’•’≥’•÷Ä’∏’æ ’§’°’ø’°’Ø’°’∂ ’Æ’°’≠’Ω’•÷Ä’®\\n‘±’∑’≠’°’ø’°’∂÷Ñ’°’µ’´’∂ ’æ’•’≥’•÷Ä’∏’æ ’§’°’ø’°’Ø’°’∂ ’Æ’°’≠’Ω’•÷Ä’® ’Ø’°’ø’°÷Ä’æ’∏÷Ç’¥ ’•’∂ ÷Ö÷Ä’•’∂÷Ñ’∏’æ ’Ω’°’∞’¥’°’∂’æ’°’Æ ’Ø’°÷Ä’£’∏’æ:\\n’Ä’°’µ’°’Ω’ø’°’∂’´ ’Ä’°’∂÷Ä’°’∫’•’ø’∏÷Ç’©’µ’°’∂\\n’å. ’î’∏’π’°÷Ä’µ’°’∂\\n’Ü’°’≠’°’£’°’∞\\n2004 ’©. ’§’•’Ø’ø’•’¥’¢’•÷Ä’´ 14\\n‘µ÷Ä÷á’°’∂\\n’Ä’ï-124-’Ü\\n¬© 1996 - 2026, ‘ª’ê’è‘µ‘ø 86 PDF -’® ’Ω’ø’•’≤’Æ’æ’°’Æ ’ß. 11.01.2026'"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text[-500:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Analyze Document Structure - Find Articles**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üìù Document Structure:\n",
            "   - Total Articles: 286\n",
            "   - Total Chapters: 25\n"
          ]
        }
      ],
      "source": [
        "article_pattern = r'’Ä’∏’§’æ’°’Æ\\s+\\d+'\n",
        "chapter_pattern = r'‘≥‘º’à’í‘Ω\\s+[IVXLCDM]+|‘≥’¨’∏÷Ç’≠\\s+\\d+'\n",
        "\n",
        "articles = re.findall(article_pattern, text, re.IGNORECASE)\n",
        "chapters = re.findall(chapter_pattern, text, re.IGNORECASE)\n",
        "\n",
        "print(f\"\\nüìù Document Structure:\")\n",
        "print(f\"   - Total Articles: {len(articles)}\")\n",
        "print(f\"   - Total Chapters: {len(chapters)}\")\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìä Line Statistics:\n",
            "   - Total lines: 4399\n",
            "   - Non-empty lines: 4314\n",
            "   - Empty lines: 85\n",
            "\n",
            "üìä Word Statistics:\n",
            "   - Total words: 39576\n",
            "   - Average word length: 7.49 chars\n",
            "   - Shortest word: 1 chars\n",
            "   - Longest word: 27 chars\n",
            "\n",
            "üìä Line Statistics:\n",
            "   - Average line length: 76.84 chars\n",
            "   - Shortest line: 4 chars\n",
            "   - Longest line: 192 chars\n"
          ]
        }
      ],
      "source": [
        "# Split into lines and analyze\n",
        "lines = text.split('\\n')\n",
        "non_empty_lines = [line for line in lines if line.strip()]\n",
        "\n",
        "print(f\"üìä Line Statistics:\")\n",
        "print(f\"   - Total lines: {len(lines)}\")\n",
        "print(f\"   - Non-empty lines: {len(non_empty_lines)}\")\n",
        "print(f\"   - Empty lines: {len(lines) - len(non_empty_lines)}\")\n",
        "\n",
        "# Word length distribution\n",
        "words = text.split()\n",
        "word_lengths = [len(word) for word in words]\n",
        "\n",
        "print(f\"\\nüìä Word Statistics:\")\n",
        "print(f\"   - Total words: {len(words)}\")\n",
        "print(f\"   - Average word length: {sum(word_lengths) / len(word_lengths):.2f} chars\")\n",
        "print(f\"   - Shortest word: {min(word_lengths)} chars\")\n",
        "print(f\"   - Longest word: {max(word_lengths)} chars\")\n",
        "\n",
        "# Line length distribution\n",
        "line_lengths = [len(line) for line in non_empty_lines]\n",
        "if line_lengths:\n",
        "    print(f\"\\nüìä Line Statistics:\")\n",
        "    print(f\"   - Average line length: {sum(line_lengths) / len(line_lengths):.2f} chars\")\n",
        "    print(f\"   - Shortest line: {min(line_lengths)} chars\")\n",
        "    print(f\"   - Longest line: {max(line_lengths)} chars\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üî§ Character Analysis:\n",
            "   - Unique characters: 115\n",
            "   - Armenian characters: 76\n",
            "   - Latin characters: 16\n",
            "   - Digits: 10\n",
            "   - Special characters: 11\n",
            "\n",
            "   Latin characters found: DFNPaehikmoprstw\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Analyze unique characters (check for encoding issues)\n",
        "unique_chars = set(text)\n",
        "print(f\"üî§ Character Analysis:\")\n",
        "print(f\"   - Unique characters: {len(unique_chars)}\")\n",
        "\n",
        "# Separate Armenian, Latin, numbers, special chars\n",
        "armenian_chars = [c for c in unique_chars if '\\u0530' <= c <= '\\u058F']\n",
        "latin_chars = [c for c in unique_chars if c.isalpha() and c.isascii()]\n",
        "digits = [c for c in unique_chars if c.isdigit()]\n",
        "special = [c for c in unique_chars if not c.isalnum() and not c.isspace()]\n",
        "\n",
        "print(f\"   - Armenian characters: {len(armenian_chars)}\")\n",
        "print(f\"   - Latin characters: {len(latin_chars)}\")\n",
        "print(f\"   - Digits: {len(digits)}\")\n",
        "print(f\"   - Special characters: {len(special)}\")\n",
        "\n",
        "if latin_chars:\n",
        "    print(f\"\\n   Latin characters found: {''.join(sorted(set(latin_chars)))}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üî¢ Most common numbers (might be page numbers or article references):\n",
            "   1: appears 566 times\n",
            "   2: appears 410 times\n",
            "   3: appears 283 times\n",
            "   06: appears 196 times\n",
            "   10: appears 194 times\n",
            "   24: appears 183 times\n",
            "   2026: appears 177 times\n",
            "   05: appears 170 times\n",
            "   4: appears 169 times\n",
            "   23: appears 154 times\n",
            "   03: appears 153 times\n",
            "   01: appears 143 times\n",
            "   160: appears 139 times\n",
            "   117: appears 125 times\n",
            "   11: appears 122 times\n",
            "   5: appears 113 times\n",
            "   22: appears 100 times\n",
            "   12: appears 87 times\n",
            "   1996: appears 86 times\n",
            "   6: appears 76 times\n",
            "\n",
            "üìÑ Found 0 lines with only numbers (potential page numbers)\n"
          ]
        }
      ],
      "source": [
        "# Look for numbers (article numbers, page numbers, etc.)\n",
        "numbers = re.findall(r'\\d+', text)\n",
        "number_counter = Counter(numbers)\n",
        "\n",
        "print(\"üî¢ Most common numbers (might be page numbers or article references):\")\n",
        "for num, count in number_counter.most_common(20):\n",
        "    print(f\"   {num}: appears {count} times\")\n",
        "\n",
        "# Find lines with only numbers (likely page numbers to remove)\n",
        "page_number_lines = [line.strip() for line in lines if line.strip().isdigit()]\n",
        "print(f\"\\nüìÑ Found {len(page_number_lines)} lines with only numbers (potential page numbers)\")\n",
        "if page_number_lines[:10]:\n",
        "    print(f\"   Examples: {page_number_lines[:10]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Identify cleaning needs**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   ‚ú® Text looks clean! Minimal cleaning needed.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "issues = []\n",
        "\n",
        "# Check for page numbers\n",
        "if page_number_lines:\n",
        "    issues.append(f\"‚úì Remove {len(page_number_lines)} standalone page numbers\")\n",
        "\n",
        "# Check for extra whitespace\n",
        "extra_spaces = len([line for line in lines if '  ' in line])\n",
        "if extra_spaces > 0:\n",
        "    issues.append(f\"‚úì Fix {extra_spaces} lines with extra whitespace\")\n",
        "\n",
        "# Check for very short lines (might be fragments)\n",
        "short_lines = [line for line in non_empty_lines if len(line) < 10]\n",
        "if len(short_lines) > 100:\n",
        "    issues.append(f\"‚úì Review {len(short_lines)} very short lines (< 10 chars)\")\n",
        "\n",
        "# Check encoding\n",
        "if len(armenian_chars) < 30:\n",
        "    issues.append(\"‚ö†Ô∏è  Warning: Very few Armenian characters detected - check encoding!\")\n",
        "\n",
        "if issues:\n",
        "    for issue in issues:\n",
        "        print(f\"   {issue}\")\n",
        "else:\n",
        "    print(\"   ‚ú® Text looks clean! Minimal cleaning needed.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
